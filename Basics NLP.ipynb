{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n#http://www.nltk.org/book/\n\nimport nltk",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0162a528a43642d6536d05fdf7adf5cc32e24e2f"
      },
      "cell_type": "markdown",
      "source": "**Tokenizing** :-\ngrouping article and paragraph into sentences and words. For this we use word tokenizer and sentence tokenizer.\n\nEg:- 'My name is Mr. Joseph. I am from Kerala, India. And I am proud to be an Indian.\n\n*sentence tokenizer o/p* :- ['My name is Mr. Joseph.', 'I am from Kerala, India.', 'And I am proud to be an Indian']\n*word tokenizer o/p* :- ['My', 'name', 'is', 'Mr.', 'Joseph', '.', 'I', 'am', 'from', 'Kerala', ',', 'India', '.', 'And', 'I', 'am', 'proud', 'to', 'be', 'an', 'Indian']\n"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.tokenize import sent_tokenize, word_tokenize\nexample_text=\"My name is Mr. Joseph. I am from Kerala, India. And I am proud to be an Indian\"\nprint(sent_tokenize(example_text))\nprint(word_tokenize(example_text))\n\nfor i in word_tokenize(example_text):     #using for loop extracting word by word\n     print(i)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3a5e14d61d5a668f528166f785042be76c475774"
      },
      "cell_type": "markdown",
      "source": "**Stopwords**:- They are set of words that are not much useful regarding the subject. \n\nEg:-  'from', 'an' , 'the', 'to'.......\n    "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "461caf2851c0d0131fbbb90d492602664568bad9"
      },
      "cell_type": "code",
      "source": "from nltk.corpus import stopwords\n\nstop_words= set(stopwords.words(\"english\"))\n#print(stop_words)       can see full list of stop words in english\nwords= word_tokenize(example_text)\n\n#using for loop\nfiltered_sentence=[]\nfor w in words:\n    if w not in stop_words:\n        filtered_sentence.append(w)\nprint(filtered_sentence)\n\n# finishing in one line\nfiltered_sentence= [ w for w in words if w not in stop_words]\nprint(filtered_sentence)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2abdaeb1da16219a703dc065a854e27fb54587db"
      },
      "cell_type": "markdown",
      "source": "**Stemming**:- its like normalising redundant words\neg:-  ridding -> rid\n**why stemming is required**? to reduce the redundancy of similar meaning sentences. Hence reducing space and wastage of memory.\nEg:- I was taking a ride in car   =  I was riding in the car\n\nFor stemming, we use **Potterstem**. Potterstem is an algorithm already in nltk for stemming. Stemming is not very important since nowadays we use wordnet.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c930f535682649f8da0907e7dad7b697c8548190"
      },
      "cell_type": "code",
      "source": "from nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nps= PorterStemmer()\nexample_words= [\"python\",\"pythoner\",\"pythonize\",\"pythoned\",\"pythonly\"] #trying in a list of words\nfor w in example_words:\n    print(ps.stem(w))\n    \nnew_text= \"It is very important to be pythonly while you are pythoning with python\" #trying for a sentence\nwords= word_tokenize(new_text)\nfor w in words:\n    print(ps.stem(w))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "55bb218d601272d765b511d63be5771970cba2a8"
      },
      "cell_type": "markdown",
      "source": "**Part of Speech Tagging**\n\nIt labels the part  of speech to words/sentences that one is feeding\n**PunktSentenceTokenizer** :- unsupervised ML sentence tokenizer. It comes pretrained. You can train it if you want.\nThis tokenizer divides a text into a list of sentences\nby using an unsupervised algorithm to build a model for abbreviation\nwords, collocations, and words that start sentences.  It must be\ntrained on a large collection of plaintext in the target language\nbefore it can be used.\n\nThe NLTK data package includes a pre-trained Punkt tokenizer for\nEnglish.\n**Corpas**:- Large body of text that are already present in nltk.\n\nPOS tag list:\n\nCC\tcoordinating conjunction\nCD\tcardinal digit\nDT\tdeterminer\nEX\texistential there (like: \"there is\" ... think of it like \"there exists\")\nFW\tforeign word\nIN\tpreposition/subordinating conjunction\nJJ\tadjective\t'big'\nJJR\tadjective, comparative\t'bigger'\nJJS\tadjective, superlative\t'biggest'\nLS\tlist marker\t1)\nMD\tmodal\tcould, will\nNN\tnoun, singular 'desk'\nNNS\tnoun plural\t'desks'\nNNP\tproper noun, singular\t'Harrison'\nNNPS\tproper noun, plural\t'Americans'\nPDT\tpredeterminer\t'all the kids'\nPOS\tpossessive ending\tparent's\nPRP\tpersonal pronoun\tI, he, she\nPRP$\tpossessive pronoun\tmy, his, hers\nRB\tadverb\tvery, silently,\nRBR\tadverb, comparative\tbetter\nRBS\tadverb, superlative\tbest\nRP\tparticle\tgive up\nTO\tto\tgo 'to' the store.\nUH\tinterjection\terrrrrrrrm\nVB\tverb, base form\ttake\nVBD\tverb, past tense\ttook\nVBG\tverb, gerund/present participle\ttaking\nVBN\tverb, past participle\ttaken\nVBP\tverb, sing. present, non-3d\ttake\nVBZ\tverb, 3rd person sing. present\ttakes\nWDT\twh-determiner\twhich\nWP\twh-pronoun\twho, what\nWP$\tpossessive wh-pronoun\twhose\nWRB\twh-abverb\twhere, when"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4b13fc60ea7737f09b5c5939f8ab4d1f0fe82c87"
      },
      "cell_type": "code",
      "source": "import nltk\nfrom nltk.corpus import state_union\nfrom nltk.tokenize import PunktSentenceTokenizer\n\ntrain_text= state_union.raw(\"2005-GWBush.txt\")\nsample_text= state_union.raw(\"2006-GWBush.txt\")\n\n\ncustom_tokenizer= PunktSentenceTokenizer(train_text)\ntokenized= custom_tokenizer.tokenize(sample_text)\n#tokenized= sent_tokenize(train_text)\ndef process_content():\n     try :\n        for i in tokenized:\n            words= nltk.word_tokenize(i)\n            tagged= nltk.pos_tag(words)\n            print(tagged)\n     except Exception as e:\n            print(str(e))\n\nprocess_content()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2ddd8e08ccbf7e81a620d8f96a84790a2987e1fc"
      },
      "cell_type": "code",
      "source": "#Chunking; higher level grouping of words\ndef process_content():\n     try :\n        for i in tokenized:\n            words= nltk.word_tokenize(i)\n            tagged= nltk.pos_tag(words)\n            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n            chunkParser= nltk.RegexpParser(chunkGram)\n            chunked= chunkParser.parse(tagged)\n            print(chunked)\n            #chunked.draw()\n            \n     except Exception as e:\n            print(str(e))\n\nprocess_content()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ff6510479485e2bd6714735f0cf96831334f3e94"
      },
      "cell_type": "code",
      "source": "#Chinking\ndef process_content():\n     try :\n        for i in tokenized:\n            words= nltk.word_tokenize(i)\n            tagged= nltk.pos_tag(words)\n            chunkGram = r\"\"\"Chunk: {<.*>+}\n                                    }<VB.?|IN|DT|TO>+{\"\"\"\n            chunkParser= nltk.RegexpParser(chunkGram)\n            chunked= chunkParser.parse(tagged)\n            print(chunked)\n            #chunked.draw()\n            \n     except Exception as e:\n            print(str(e))\n\nprocess_content()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3697ee4a275ea6f45548e45b4ece2ff6c162d515"
      },
      "cell_type": "code",
      "source": "#Named Entity\ndef process_content():\n     try :\n        for i in tokenized:\n            words= nltk.word_tokenize(i)\n            tagged= nltk.pos_tag(words)\n            namedEnt= nltk.ne_chunk(tagged, binary=True)\n            print(namedEnt)\n     except Exception as e:\n            print(str(e))\n\nprocess_content()",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}