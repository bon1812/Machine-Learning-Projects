{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearningAssignment1.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "ZQwrOtBxnB5N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7o-jQXyYKEuA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url = \"https://drive.google.com/uc?export=download&id=1-OYVv-1-caiUVmcBY6PC8_80zyiG7wGl\"\n",
        "df = pd.read_excel(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QxVPsFC5oPYv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Make the data frame. And divide into predictors and target variables\n",
        "\n",
        "predictors = df[['Q1','Q2pt5','Q5','Q10','Q25','Q50','Q75','Q90','Q95','Q97pt5','Q99','Q100']]\n",
        "target= df[['POPESTIMATE2015']]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_wb5kVamyVCC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#correlation matrix\n",
        "\n",
        "corrmat = predictors.corr()\n",
        "f, ax = plt.subplots(figsize=(12, 9))\n",
        "sns.heatmap(corrmat, annot= True,vmax=.9, square=True);  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ro8plbc-swES",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#pairplot\n",
        "sns.pairplot(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FPoeYc1Ky9yy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Now Lets divide the dataset into training and testing data**"
      ]
    },
    {
      "metadata": {
        "id": "tG6PLaA6yJEp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(predictors, target , test_size=0.25, random_state=123) \n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PMws_L_BzMFD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Linear Regression**"
      ]
    },
    {
      "metadata": {
        "id": "EQedM97KyJUB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Fit Linear Regression\n",
        "lm = LinearRegression()\n",
        "lm.fit(X_train,y_train)\n",
        "\n",
        "y_pred = lm.predict(X_test)\n",
        "y_pred_train= lm.predict(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZLiK_5YaG0EX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate MAE\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "linear_MAE_test= mean_absolute_error(y_test, y_pred)\n",
        "print('Mean Absolute Error for test dataset:', round(np.mean(linear_MAE_test), 2))\n",
        "linear_MAE_train= mean_absolute_error(y_train, y_pred_train)\n",
        "print('Mean Absolute Error for train dataset:', round(np.mean(linear_MAE_train), 2))\n",
        "\n",
        "\n",
        "# Calculate MAPE\n",
        "\n",
        "def mean_absolute_percentage_error(y_test, y_pred): \n",
        "    y_test, y_pred = np.array(y_test), np.array(y_pred)\n",
        "    return round(np.mean(np.abs((y_test - y_pred) / y_test)) * 100 ,2)\n",
        "\n",
        "Linear_MAPE_test=mean_absolute_percentage_error(y_test, y_pred)\n",
        "print('Mean Absolute Percentage Error for test dataset:', Linear_MAPE_test)\n",
        "Linear_MAPE_train=mean_absolute_percentage_error(y_train, y_pred_train)\n",
        "print('Mean Absolute Percentage Error for training dataset:', Linear_MAPE_train)\n",
        "\n",
        "# Calculate R square\n",
        "\n",
        "Linear_r2_test= r2_score(y_test, y_pred)\n",
        "print('R^2 for test dataset:', round(np.mean(Linear_r2_test), 2))\n",
        "Linear_r2_train= r2_score(y_train, y_pred_train) \n",
        "print('R^2 for train dataset:', round(np.mean(Linear_r2_train), 2))\n",
        "\n",
        "#Calculate RMSE\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "Linear_rms_test = sqrt(mean_squared_error(y_test, y_pred))\n",
        "print('RMSE for test dataset:', round(np.mean(Linear_rms_test), 2))\n",
        "Linear_rms_train = sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "print('RMSE for train dataset:', round(np.mean(Linear_rms_train), 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TjrXJzlizUKS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Random Forest**\n",
        "\n",
        "Create an instance of the GridSearchCV\n",
        "\n",
        "You need to pass values for the estimator parameter, which basically is the algorithm that you want to execute.The **param_grid** parameter takes the parameter dictionary that we just created as parameter, the **scoring** parameter takes the performance metrics, the **cv** parameter corresponds to number of folds, which is 10 in our case, and finally the **n_jobs** parameter refers to the number of CPU's that you want to use for execution. A value of -1 for n_jobs parameter means that use all available computing power. This can be handy if you have large number amount of data.\n",
        "\n",
        "**n_estimators** is the number of trees to be used in the forest. **max_features** on the other hand, determines the maximum number of features to consider while looking for a split"
      ]
    },
    {
      "metadata": {
        "id": "En3wLCqbSHBt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
        "#https://stackabuse.com/cross-validation-and-grid-search-for-model-selection-in-python/\n",
        "#https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'bootstrap': [True],\n",
        "    'max_depth': [80,100],\n",
        "    'max_features': [2,3],\n",
        "    'min_samples_leaf': [3,5],\n",
        "    'min_samples_split': [8,12],\n",
        "    'n_estimators': [100, 500, 900]  #can use this also- range(20,81,10)\n",
        "    \n",
        "}\n",
        "\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator = RandomForestRegressor(random_state=123), param_grid = param_grid,   \n",
        "                          cv = 10, n_jobs = -1, verbose = 2)\n",
        "grid_search.fit(X_train,y_train)\n",
        "grid_search.best_params_\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JOoyeov3Ud3G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "4977a5f1-1e02-433c-cfe5-58bd84e64b75"
      },
      "cell_type": "code",
      "source": [
        "#Fit the Random Forest with the best parameters\n",
        "rf= RandomForestRegressor(random_state=123, bootstrap=True,max_depth=80,max_features=2,min_samples_leaf=5,min_samples_split=8,n_estimators=900)\n",
        "rf.fit(X_train, y_train);\n",
        "\n",
        "y_pred = rf.predict(X_test)\n",
        "y_pred_train= rf.predict(X_train)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "wXdy97ycyJgF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Calculate MAE\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "RF_MAE_test= mean_absolute_error(y_test, y_pred)\n",
        "print('Mean Absolute Error for test dataset:', round(np.mean(linear_MAE_test), 2))\n",
        "RF_MAE_train= mean_absolute_error(y_train, y_pred_train)\n",
        "print('Mean Absolute Error for train dataset:', round(np.mean(linear_MAE_train), 2))\n",
        "\n",
        "\n",
        "# Calculate MAPE\n",
        "\n",
        "def mean_absolute_percentage_error(y_test, y_pred): \n",
        "    y_test, y_pred = np.array(y_test), np.array(y_pred)\n",
        "    return round(np.mean(np.abs((y_test - y_pred) / y_test)) * 100 ,2)\n",
        "\n",
        "RF_MAPE_test=mean_absolute_percentage_error(y_test, y_pred)\n",
        "print('Mean Absolute Percentage Error for test dataset:', RF_MAPE_test)\n",
        "RF_MAPE_train=mean_absolute_percentage_error(y_train, y_pred_train)\n",
        "print('Mean Absolute Percentage Error for training dataset:', RF_MAPE_training)\n",
        "\n",
        "# Calculate R square\n",
        "\n",
        "RF_r2_test= r2_score(y_test, y_pred)\n",
        "print('R^2 for test dataset:', round(np.mean(RF_r2_test), 2))\n",
        "RF_r2_train= r2_score(y_train, y_pred_train) \n",
        "print('R^2 for train dataset:', round(np.mean(RF_r2_train), 2))\n",
        "\n",
        "#Calculate RMSE\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "RF_rms_test = sqrt(mean_squared_error(y_test, y_pred))\n",
        "print('RMSE for test dataset:', round(np.mean(RF_rms_test), 2))\n",
        "RF_rms_train = sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "print('RMSE for train dataset:', round(np.mean(RF_rms_train), 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FC6ugqp50Oj2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Gradient Boosting Regressor**\n",
        "\n",
        "Here also we utilize the GridSearch"
      ]
    },
    {
      "metadata": {
        "id": "OVy-455otK8z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#https://www.datacareer.ch/blog/parameter-tuning-in-gradient-boosting-gbm-with-python/\n",
        "#https://stackabuse.com/cross-validation-and-grid-search-for-model-selection-in-python/\n",
        "#https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "\n",
        "#can add learning rate also with values between 0.1 and 0.2\n",
        "param_grid = {\n",
        "     \n",
        "    'max_depth': [80,100],\n",
        "    'max_features': [2, 3],\n",
        "    'min_samples_leaf': [3,5],\n",
        "    'min_samples_split': [8,12],\n",
        "    'n_estimators': [100,900]  #can use ths also- range(20,81,10)\n",
        "    \n",
        "}\n",
        "\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator = GradientBoostingRegressor(random_state=123), param_grid = param_grid,   \n",
        "                          cv = 10, n_jobs = -1, verbose = 2)\n",
        "grid_search.fit(X_train,y_train)\n",
        "grid_search.best_params_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Yse2T9MQUcH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Fit the GradientBoostingRegressor with the best parameters\n",
        "gbm= GradientBoostingRegressor(random_state=123,max_depth=80,max_features=3,min_samples_leaf=3,min_samples_split=8,n_estimators=100)\n",
        "gbm.fit(X_train, y_train);\n",
        "\n",
        "y_pred = gbm.predict(X_test)\n",
        "y_pred_train= gbm.predict(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EQJDLix3qLfB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Calculate MAE\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "GBM_MAE_test= mean_absolute_error(y_test, y_pred)\n",
        "print('Mean Absolute Error for test dataset:', round(np.mean(GBM_MAE_test), 2))\n",
        "GBM_MAE_train= mean_absolute_error(y_train, y_pred_train)\n",
        "print('Mean Absolute Error for train dataset:', round(np.mean(GBM_MAE_train), 2))\n",
        "\n",
        "\n",
        "# Calculate MAPE\n",
        "\n",
        "def mean_absolute_percentage_error(y_test, y_pred): \n",
        "    y_test, y_pred = np.array(y_test), np.array(y_pred)\n",
        "    return round(np.mean(np.abs((y_test - y_pred) / y_test)) * 100 ,2)\n",
        "\n",
        "GBM_MAPE_test=mean_absolute_percentage_error(y_test, y_pred)\n",
        "print('Mean Absolute Percentage Error for test dataset:', GBM_MAPE_test)\n",
        "GBM_MAPE_train=mean_absolute_percentage_error(y_train, y_pred_train)\n",
        "print('Mean Absolute Percentage Error for training dataset:', GBM_MAPE_training)\n",
        "\n",
        "# Calculate R square\n",
        "\n",
        "GBM_r2_test= r2_score(y_test, y_pred)\n",
        "print('R^2 for test dataset:', round(np.mean(GBM_r2_test), 2))\n",
        "GBM_r2_train= r2_score(y_train, y_pred_train) \n",
        "print('R^2 for train dataset:', round(np.mean(GBM_r2_train), 2))\n",
        "\n",
        "#Calculate RMSE\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "GBM_rms_test = sqrt(mean_squared_error(y_test, y_pred))\n",
        "print('RMSE for test dataset:', round(np.mean(GBM_rms_test), 2))\n",
        "GBM_rms_train = sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "print('RMSE for train dataset:', round(np.mean(GBM_rms_train), 2))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sVmoZqLm2Fk3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now Lets Create a table that has the MAPE (sd) and MAE (sd) of each model (training and validation\n",
        "results, please)."
      ]
    },
    {
      "metadata": {
        "id": "BE5ATUgcZbSH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "I = pd.Index([\"Linear\", \"RandomForest\", \"GradientBoosting\"], name=\"rows\")\n",
        "C = pd.Index([\"Training MAE \",\"Testing MAE\", \"Training MAPE\",\"Testing MAPE\"], name=\"columns\")\n",
        "ModelPerformance = [ (linear_MAE_train, linear_MAE_test, Linear_MAPE_train, Linear_MAPE_test) ,\n",
        "             (RF_MAE_train, RF_MAE_test,RF_MAPE_train, RF_MAPE_test ) ,\n",
        "             (GBM_MAE_train, GBM_MAE_test,GBM_MAPE_train, GBM_MAPE_test  ) \n",
        "               ]\n",
        "dfError = pd.DataFrame(ModelPerformance, index=I, columns=C)\n",
        "dfError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PzFhlWnEmNU7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Though Linear model has low testing MAPE, it has high tesing MAE.\n",
        "Further for random forest, the difference between training and testing MAE is huge and tesing MAPE is more than Gradient Boosting. Hence Looks like Gradient Boosting is a better model among these. So I choose Gradient Boosting"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JLxONkqSuvmr",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Predicting the worst 50 predicted counties\n",
        "\n",
        "y_pred = y_pred.reshape(625,1)\n",
        "BestModelMAE= round(abs(y_test- y_pred),2)\n",
        "df['MAE']= BestModelMAE\n",
        "BestModelMAPE= round(abs((y_test - y_pred) / y_test),2)\n",
        "df['MAPE']= BestModelMAPE\n",
        "\n",
        "df.sort_values(by='MAPE', ascending=False, na_position='last', inplace= True)\n",
        "dfTop50= df.CTYNAME.head(50)\n",
        "dfTop50\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k5QTkiV8c1xA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Boxplot to show MAE for Texas, Florida, California\n",
        "df1= pd.concat([df[df.STNAME == 'Texas'], df[df.STNAME == 'Florida'],df[df.STNAME == 'California']])\n",
        "df1.groupby('STNAME')\n",
        "f, ax = plt.subplots(figsize=(8, 6))\n",
        "fig = sns.boxplot(x='STNAME', y=\"MAE\", data=df1)\n",
        "fig.axis(ymin=0, ymax=1000000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U-GtH15Oc6Xt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Boxplot to show MAPE for Texas, Florida, California\n",
        "fig = sns.boxplot(x='STNAME', y=\"MAPE\", data=df1)\n",
        "fig.axis(ymin=0, ymax=5)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}