{
  "cells": [
    {
      "metadata": {
        "_uuid": "1c52babf6e27c44e024542a608cb0b247a50f426"
      },
      "cell_type": "markdown",
      "source": "**Sentiment Analysis**: the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.classify import SklearnClassifier\nfrom wordcloud import WordCloud,STOPWORDS\n\nfrom sklearn.model_selection import train_test_split \n\n#from subprocess import check_output\n\nimport os\nprint(os.listdir(\"../input\"))\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5257bc4d737d8cbbdbc17115ae40bf99d828916f"
      },
      "cell_type": "code",
      "source": " #droppig the unnecessary colunns, keeping only sentiment and text\ndata = pd.read_csv('../input/Sentiment.csv')\n# Keeping only the neccessary columns\ndata = data[['text','sentiment']]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Splitting the dataset into train and test set\ntrain, test = train_test_split(data,test_size = 0.1)\n# Removing neutral sentiments\ntrain = train[train.sentiment != \"Neutral\"]\n\n#separate the Positive and Negative tweets of the training set\ntrain_pos = train[ train['sentiment'] == 'Positive']\ntrain_pos = train_pos['text']\ntrain_neg = train[ train['sentiment'] == 'Negative']\ntrain_neg = train_neg['text']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3fb9a04c0f5802b5c3e4bd444d440c4d880e8e7c"
      },
      "cell_type": "code",
      "source": "train_pos",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c59cfce9cd7f2bb7617f4732f70ee979177aba10"
      },
      "cell_type": "code",
      "source": "#wordcloud\ndef wordcloud_draw(data, color):\n    words = ' '.join(data)       #join function to join list elements without any separator. \n    cleaned_word = \" \".join([word for word in words.split()   #splitting by a space\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and not word.startswith('#')\n                                and word != 'RT'\n                            ])\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color=color,\n                      width=2500,\n                      height=2000\n                     ).generate(cleaned_word)\n    plt.figure(1,figsize=(13, 13))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n    \nprint(\"Positive words\")\nwordcloud_draw(train_pos,'white')\nprint(\"Negative words\")\nwordcloud_draw(train_neg, 'black')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "583b762f521653cc467826d6a2708158307cf264"
      },
      "cell_type": "code",
      "source": "tweets = []\nstopwords_set = set(stopwords.words(\"english\"))\n\nfor index, row in train.iterrows():\n    words_filtered = [e.lower() for e in row.text.split() if len(e) >= 3]\n    words_cleaned = [word for word in words_filtered\n        if 'http' not in word\n        and not word.startswith('@')\n        and not word.startswith('#')\n        and word != 'RT']\n    words_without_stopwords = [word for word in words_cleaned if not word in stopwords_set]\n    tweets.append((words_without_stopwords, row.sentiment))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a603db68b78e51c24771961d8afd08541bcd6a36"
      },
      "cell_type": "code",
      "source": "# Extracting word features\ndef get_words_in_tweets(tweets):\n    all = []\n    for (words, sentiment) in tweets:\n        all.extend(words)\n    return all\n\ndef get_word_features(wordlist):\n    wordlist = nltk.FreqDist(wordlist)\n    features = wordlist.keys()\n    return features\nw_features = get_word_features(get_words_in_tweets(tweets))\n\ndef extract_features(document):\n    document_words = set(document)\n    features = {}\n    for word in w_features:\n        features['contains(%s)' % word] = (word in document_words)\n    return features",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "85100151904061e1c6bba55c735c7608ab0c81c4"
      },
      "cell_type": "code",
      "source": "#plotted the most frequently distributed words. The most words are centered around debate nights.\nwordcloud_draw(w_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1260aa17d4e8ab557c1c270da4ade9e6c7f8d9df"
      },
      "cell_type": "code",
      "source": "test_pos = test[ test['sentiment'] == 'Positive']\ntest_pos = test_pos['text']\ntest_neg = test[ test['sentiment'] == 'Negative']\ntest_neg = test_neg['text']\n\n# Training the Naive Bayes classifier\ntraining_set = nltk.classify.apply_features(extract_features,tweets)\nclassifier = nltk.NaiveBayesClassifier.train(training_set)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c4416975d27f883d005374029407db1f21c1222"
      },
      "cell_type": "code",
      "source": "#not so intelligent metrics\nneg_cnt = 0\npos_cnt = 0\nfor obj in test_neg: \n    res =  classifier.classify(extract_features(obj.split()))\n    if(res == 'Negative'): \n        neg_cnt = neg_cnt + 1\nfor obj in test_pos: \n    res =  classifier.classify(extract_features(obj.split()))\n    if(res == 'Positive'): \n        pos_cnt = pos_cnt + 1\n        \nprint('[Negative]: %s/%s '  % (len(test_neg),neg_cnt))        \nprint('[Positive]: %s/%s '  % (len(test_pos),pos_cnt))    ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}